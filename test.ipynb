{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torchaudio_sox::load_audio_file() Expected a value of type 'str' for argument '_0' but instead found type 'PosixPath'.\nPosition: 0\nValue: PosixPath('audio_cache/test.wav')\nDeclaration: torchaudio_sox::load_audio_file(str _0, int? _1, int? _2, bool? _3, bool? _4, str? _5) -> (Tensor _0, int _1)\nCast error details: Unable to cast Python instance of type <class 'pathlib.PosixPath'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/caoyujia/Desktop/BLINC/test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest.wav\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Make sure this path is a string\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Use model to separate the audio file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m enhanced_speech \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mseparate_file(path\u001b[39m=\u001b[39mpath)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Use IPython.display.Audio to play the enhanced speech\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m Audio(enhanced_speech[:, :]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze(), rate\u001b[39m=\u001b[39m\u001b[39m8000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/inference/separation.py:100\u001b[0m, in \u001b[0;36mSepformerSeparation.separate_file\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m     97\u001b[0m source, fl \u001b[39m=\u001b[39m split_path(path)\n\u001b[1;32m     98\u001b[0m path \u001b[39m=\u001b[39m fetch(fl, source\u001b[39m=\u001b[39msource, savedir\u001b[39m=\u001b[39msavedir)\n\u001b[0;32m--> 100\u001b[0m batch, fs_file \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(path)\n\u001b[1;32m    101\u001b[0m batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    102\u001b[0m fs_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39msample_rate\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[39mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[39m=\u001b[39m dispatcher(uri, \u001b[39mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchaudio/_backend/sox.py:44\u001b[0m, in \u001b[0;36mSoXBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSoX backend does not support loading from file-like objects. \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use an alternative backend that does support loading from file-like objects, e.g. FFmpeg.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     ret \u001b[39m=\u001b[39m sox_ext\u001b[39m.\u001b[39mload_audio_file(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ret:\n\u001b[1;32m     46\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load audio from \u001b[39m\u001b[39m{\u001b[39;00muri\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(kwargs \u001b[39mor\u001b[39;00m {}))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torchaudio_sox::load_audio_file() Expected a value of type 'str' for argument '_0' but instead found type 'PosixPath'.\nPosition: 0\nValue: PosixPath('audio_cache/test.wav')\nDeclaration: torchaudio_sox::load_audio_file(str _0, int? _1, int? _2, bool? _3, bool? _4, str? _5) -> (Tensor _0, int _1)\nCast error details: Unable to cast Python instance of type <class 'pathlib.PosixPath'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Initialize the model\n",
    "model = separator.from_hparams(source=\"speechbrain/sepformer-whamr-enhancement\", savedir='pretrained_models/sepformer-whamr-enhancement')\n",
    "\n",
    "# Provide the path to the audio file\n",
    "path = 'test.wav'  # Make sure this path is a string\n",
    "\n",
    "# Use model to separate the audio file\n",
    "enhanced_speech = model.separate_file(path=path)\n",
    "\n",
    "# Use IPython.display.Audio to play the enhanced speech\n",
    "Audio(enhanced_speech[:, :].detach().cpu().squeeze(), rate=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/caoyujia/Desktop/BLINC/test.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspeechbrain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mASR\u001b[39;00m \u001b[39mimport\u001b[39;00m EncoderDecoderASR\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m asr_model \u001b[39m=\u001b[39m EncoderDecoderASR\u001b[39m.\u001b[39mfrom_hparams(source\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mspeechbrain/asr-crdnn-rnnlm-librispeech\u001b[39m\u001b[39m\"\u001b[39m, savedir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpretrained_models/asr-crdnn-rnnlm-librispeech\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caoyujia/Desktop/BLINC/test.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m asr_model\u001b[39m.\u001b[39mtranscribe_file(\u001b[39m'\u001b[39m\u001b[39maudio/102-f8c26def-b295-4793-8d07-bf995af254dc (Fri Dec 10 15_58_59 2021)_redu.wav\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/inference/ASR.py:81\u001b[0m, in \u001b[0;36mEncoderDecoderASR.transcribe_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m batch \u001b[39m=\u001b[39m waveform\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     80\u001b[0m rel_length \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 81\u001b[0m predicted_words, predicted_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranscribe_batch(\n\u001b[1;32m     82\u001b[0m     batch, rel_length\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m predicted_words[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/inference/ASR.py:150\u001b[0m, in \u001b[0;36mEncoderDecoderASR.transcribe_batch\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         inputs \u001b[39m=\u001b[39m [encoder_out, wav_lens]\n\u001b[0;32m--> 150\u001b[0m     predicted_tokens, _, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmods\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39minputs)\n\u001b[1;32m    151\u001b[0m     predicted_words \u001b[39m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode_ids(token_seq)\n\u001b[1;32m    153\u001b[0m         \u001b[39mfor\u001b[39;00m token_seq \u001b[39min\u001b[39;00m predicted_tokens\n\u001b[1;32m    154\u001b[0m     ]\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m predicted_words, predicted_tokens\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/decoders/seq2seq.py:1318\u001b[0m, in \u001b[0;36mS2SBeamSearcher.forward\u001b[0;34m(self, enc_states, wav_len)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_full_beams(eos_hyps_and_log_probs_scores):\n\u001b[1;32m   1306\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m     (\n\u001b[1;32m   1309\u001b[0m         alived_hyps,\n\u001b[1;32m   1310\u001b[0m         inp_tokens,\n\u001b[1;32m   1311\u001b[0m         log_probs,\n\u001b[1;32m   1312\u001b[0m         eos_hyps_and_log_probs_scores,\n\u001b[1;32m   1313\u001b[0m         memory,\n\u001b[1;32m   1314\u001b[0m         scorer_memory,\n\u001b[1;32m   1315\u001b[0m         attn,\n\u001b[1;32m   1316\u001b[0m         prev_attn_peak,\n\u001b[1;32m   1317\u001b[0m         scores,\n\u001b[0;32m-> 1318\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_step(\n\u001b[1;32m   1319\u001b[0m         alived_hyps,\n\u001b[1;32m   1320\u001b[0m         inp_tokens,\n\u001b[1;32m   1321\u001b[0m         log_probs,\n\u001b[1;32m   1322\u001b[0m         eos_hyps_and_log_probs_scores,\n\u001b[1;32m   1323\u001b[0m         memory,\n\u001b[1;32m   1324\u001b[0m         scorer_memory,\n\u001b[1;32m   1325\u001b[0m         attn,\n\u001b[1;32m   1326\u001b[0m         prev_attn_peak,\n\u001b[1;32m   1327\u001b[0m         enc_states,\n\u001b[1;32m   1328\u001b[0m         enc_lens,\n\u001b[1;32m   1329\u001b[0m         step,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1332\u001b[0m finals_hyps_and_log_probs_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_alived_hyps_with_eos_token(\n\u001b[1;32m   1333\u001b[0m     alived_hyps, eos_hyps_and_log_probs_scores, scores,\n\u001b[1;32m   1334\u001b[0m )\n\u001b[1;32m   1336\u001b[0m (\n\u001b[1;32m   1337\u001b[0m     topk_hyps,\n\u001b[1;32m   1338\u001b[0m     topk_lengths,\n\u001b[1;32m   1339\u001b[0m     topk_scores,\n\u001b[1;32m   1340\u001b[0m     topk_log_probs,\n\u001b[1;32m   1341\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_topk_prediction(finals_hyps_and_log_probs_scores)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/decoders/seq2seq.py:1179\u001b[0m, in \u001b[0;36mS2SBeamSearcher.search_step\u001b[0;34m(self, alived_hyps, inp_tokens, log_probs, eos_hyps_and_log_probs_scores, memory, scorer_memory, attn, prev_attn_peak, enc_states, enc_lens, step)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_step\u001b[39m(\n\u001b[1;32m   1116\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1117\u001b[0m     alived_hyps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     step,\n\u001b[1;32m   1128\u001b[0m ):\n\u001b[1;32m   1129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A search step for the next most likely tokens.\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \n\u001b[1;32m   1131\u001b[0m \u001b[39m    Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39m        The scores of the current step output.\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     (log_probs, memory, attn,) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attn_weight_step(\n\u001b[1;32m   1180\u001b[0m         inp_tokens, memory, enc_states, enc_lens, attn, log_probs,\n\u001b[1;32m   1181\u001b[0m     )\n\u001b[1;32m   1183\u001b[0m     \u001b[39m# Keep the original value\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m     log_probs_clone \u001b[39m=\u001b[39m log_probs\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/decoders/seq2seq.py:552\u001b[0m, in \u001b[0;36mS2SBeamSearcher._attn_weight_step\u001b[0;34m(self, inp_tokens, memory, enc_states, enc_lens, attn, log_probs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This method computes a forward_step if attn_weight is superior to 0.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \n\u001b[1;32m    525\u001b[0m \u001b[39mArguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39m    The attention weight.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 552\u001b[0m     log_probs, memory, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_step(\n\u001b[1;32m    553\u001b[0m         inp_tokens, memory, enc_states, enc_lens\n\u001b[1;32m    554\u001b[0m     )\n\u001b[1;32m    555\u001b[0m     log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_weight \u001b[39m*\u001b[39m log_probs\n\u001b[1;32m    556\u001b[0m \u001b[39mreturn\u001b[39;00m log_probs, memory, attn\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/decoders/seq2seq.py:1450\u001b[0m, in \u001b[0;36mS2SRNNBeamSearcher.forward_step\u001b[0;34m(self, inp_tokens, memory, enc_states, enc_lens)\u001b[0m\n\u001b[1;32m   1448\u001b[0m hs, c \u001b[39m=\u001b[39m memory\n\u001b[1;32m   1449\u001b[0m e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(inp_tokens)\n\u001b[0;32m-> 1450\u001b[0m dec_out, hs, c, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec\u001b[39m.\u001b[39mforward_step(\n\u001b[1;32m   1451\u001b[0m     e, hs, c, enc_states, enc_lens\n\u001b[1;32m   1452\u001b[0m )\n\u001b[1;32m   1453\u001b[0m log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(dec_out) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature)\n\u001b[1;32m   1454\u001b[0m \u001b[39m# average attn weight of heads when attn_type is multiheadlocation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/nnet/RNN.py:910\u001b[0m, in \u001b[0;36mAttentionalRNNDecoder.forward_step\u001b[0;34m(self, inp, hs, c, enc_states, enc_len)\u001b[0m\n\u001b[1;32m    907\u001b[0m cell_inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(cell_inp)\n\u001b[1;32m    908\u001b[0m cell_out, hs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(cell_inp, hs)\n\u001b[0;32m--> 910\u001b[0m c, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(enc_states, enc_len, cell_out)\n\u001b[1;32m    911\u001b[0m dec_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([c, cell_out], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    912\u001b[0m dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(dec_out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/speechbrain/nnet/attention.py:219\u001b[0m, in \u001b[0;36mLocationAwareAttention.forward\u001b[0;34m(self, enc_states, enc_len, dec_states)\u001b[0m\n\u001b[1;32m    215\u001b[0m attn_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_loc(attn_conv\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m    217\u001b[0m dec_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_dec(dec_states\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m    218\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_attn(\n\u001b[0;32m--> 219\u001b[0m     torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecomputed_enc_h \u001b[39m+\u001b[39m dec_h \u001b[39m+\u001b[39m attn_conv)\n\u001b[1;32m    220\u001b[0m )\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[39m# mask the padded frames\u001b[39;00m\n\u001b[1;32m    223\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39mmasked_fill(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speechbrain as sb\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from IPython.display import Audio\n",
    "\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\")\n",
    "asr_model.transcribe_file('audio/102-f8c26def-b295-4793-8d07-bf995af254dc (Fri Dec 10 15_58_59 2021)_redu.wav')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
